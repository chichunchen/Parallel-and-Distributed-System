#!/usr/bin/env python

import argparse
from random import shuffle

try:
    import Queue as Q  # ver. < 3.0
except ImportError:
    import queue as Q


class DepNode:
    def __init__(self):
        self.time = 0
        self.ndeps = 0
        self.dep_set = set()  # set that this node depends on
        self.starttime = -1
        self.endtime = -1
        self.processor = None

    def set(self, time, ndeps):
        self.time = time
        self.ndeps = ndeps


class DepGraph:
    def __init__(self, filename):
        f = open(filename, 'r')
        firstline = f.readline().split()

        assert len(firstline) == 2
        nodes, edges = [(int(x)) for x in firstline]

        # init all instance variables
        self.nodemap = {}
        for x in range(0, nodes):
            self.nodemap[x] = DepNode()
        self.__nodes = nodes
        self.__edges = edges
        self.__deps_pool = set()
        self.__no_deps_pool = set()
        self.__done_set = set()

        # read line from file
        for line in f:
            node_arr = line.split()
            taskid = int(node_arr[0])
            time = int(node_arr[1])
            ndeps = int(node_arr[2])
            for x in node_arr[3:]:
                self.nodemap[int(x)].dep_set.add(taskid)
                self.__deps_pool.add(int(x))
            self.nodemap[taskid].set(time, ndeps)

        # assume node number is sequential and always start from zero
        for x in range(0, self.__nodes):
            self.__no_deps_pool.add(x)

        self.__no_deps_pool -= self.__deps_pool

        assert len(self.nodemap) == nodes

    def get_nodes(self):
        return self.__nodes

    # check all nodes with dependences with done_set
    # return nodes with no deps
    def new_no_deps(self, starttime):
        ret = set()
        for x in self.__deps_pool:
            temp = self.nodemap[x]
            if self.__done_set.issuperset(temp.dep_set):
                ret.add(x)
        for x in ret:
            node = self.nodemap[x]
            node.starttime = starttime
            node.endtime = starttime + node.time
        return ret

    def commit(self, no_deps):
        self.__no_deps_pool |= no_deps
        self.__deps_pool -= no_deps

    def rollback(self, unavail_set):
        self.__no_deps_pool -= unavail_set
        self.__deps_pool |= unavail_set

    def dep_pool_is_empty(self):
        return self.__deps_pool == set()

    def no_dep_pool_is_empty(self):
        return self.__no_deps_pool == set()

    # update the time quantum nodes in no dep set and return the tasks that has been done
    def update_no_dep_nodes_time(self, min_time):
        local_done_set = set()

        for x in self.__no_deps_pool:
            self.nodemap[x].time -= min_time
            if self.nodemap[x].time == 0:
                self.__done_set.add(x)
                local_done_set.add(x)

        self.__no_deps_pool -= self.__done_set

        return local_done_set

    # return the minimum time in nodemap
    def find_shortest_time(self):
        time_iter = map(lambda x: self.nodemap[x].time, self.__no_deps_pool)
        min_time = min(time_iter)

        if min_time < 0:
            min_time = 0

        return min_time

    def get_no_deps_pool(self):
        return self.__no_deps_pool


class Core:
    def __init__(self, x):
        self.execution_time = 0
        self.id = x

    def __cmp__(self, other):
        return self.execution_time - other.execution_time


class Machine:
    # assign cores desc using task_id
    # return avail_set
    def navie_assign(self, nodemap, assign_set):
        assign_iter = reversed(sorted(list(assign_set)))
        return self.base_scheduling(nodemap, assign_iter)

    # assign cores to tasks with longest executing time
    # return avail_set
    def longest_remaining_time_first(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: nodemap[x].time)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks with shortest executing time
    # return avail_set
    def shortest_remaining_time_first(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: -nodemap[x].time)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks using random task_id
    # return avail_set
    def random_schduling(self, nodemap, assign_set):
        assign_list = list(assign_set)
        shuffle(assign_list)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks with fewest deps
    def dep_based_scheduling(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: len(nodemap[x].dep_set))
        return self.base_scheduling(nodemap, assign_list)


class InfiniteResourceMachine(Machine):
    def __init__(self, n):
        self.strategy = self.shortest_remaining_time_first
        self.max = 0
        self.__cores = list(range(0, n))  # use cores as LIFO queue

    # assign one task to first core in queue
    def pop(self):
        assert len(self.__cores) > 0
        assigned_core = self.__cores.pop(0)
        if self.max < assigned_core:
            self.max = assigned_core
        return assigned_core

    # assign tasks from assign_set
    # return list of task index that has been assigned the task
    def assign_cores(self, dgraph, assign_set):
        nodemap = dgraph.nodemap
        if len(assign_set) <= len(self.__cores):
            for x in assign_set:
                p = self.pop()
                nodemap[x].processor = p
            return assign_set
        else:
            avail_set = self.strategy(nodemap, assign_set)
            unavail_set = assign_set - avail_set
            dgraph.rollback(unavail_set)

            return avail_set

    # yielding the core by inserting it back to core list
    def push(self, n):
        self.__cores.insert(0, n)
        assert self.__cores[0] >= 0

    # yield all the cores that has done their task
    def yield_cores(self, nodemap, done_set):
        for x in done_set:
            node = nodemap[x]
            self.push(node.processor)
            assert node.processor >= 0

    def core_usage(self):
        return self.max + 1  # plus one for core-0

    def base_scheduling(self, nodemap, task_queue):
        avail_set = set()
        for x in task_queue:
            if len(self.__cores) > 0:
                p = self.pop()
                nodemap[x].processor = p
                avail_set.add(x)
            else:
                break
        return avail_set


class ResourceRestrainedMachine(Machine):
    def __init__(self, n):
        self.strategy = self.shortest_remaining_time_first
        self.__cores = Q.PriorityQueue(n)
        for x in range(0, n):
            self.__cores.put(Core(x))
        self.__core_use_set = set()

    # assign one task to first core in queue
    def __pop(self):
        assert self.__cores.qsize() > 0
        assigned_core = self.__cores.get()
        if assigned_core.id not in self.__core_use_set:
            self.__core_use_set.add(assigned_core.id)
        return assigned_core

    # assign tasks from assign_set
    # return list of task index that has been assigned the task
    def assign_cores(self, dgraph, assign_set):
        nodemap = dgraph.nodemap

        if len(assign_set) <= self.__cores.qsize():
            for x in assign_set:
                core = self.__pop()
                nodemap[x].processor = core
                core.execution_time += nodemap[x].time
            return assign_set
        else:
            avail_set = self.strategy(nodemap, assign_set)
            unavail_set = assign_set - avail_set
            dgraph.rollback(unavail_set)

            return avail_set

    # yielding the core by inserting it back to core list
    def __push(self, core):
        self.__cores.put(core)
        # assert self.__cores[0] >= 0

    # yield all the cores that has done their task
    def yield_cores(self, nodemap, done_set):
        for x in done_set:
            node = nodemap[x]
            self.__push(node.processor)

    def core_usage(self):
        return len(self.__core_use_set)  # plus one for core-0

    def base_scheduling(self, nodemap, task_queue):
        avail_set = set()
        for x in task_queue:
            if self.__cores.qsize() > 0:
                core = self.__pop()
                nodemap[x].processor = core
                core.execution_time += nodemap[x].time
                avail_set.add(x)
            else:
                break
        return avail_set


class Scheduler:
    def __init__(self, dgraph, nproc):
        self.dgraph = dgraph
        self.nproc = nproc
        self.current_time = 0

        # assume that use nodes as core num is the same as use infinite nodes as core num
        if nproc == 0:
            self.machine = InfiniteResourceMachine(self.dgraph.get_nodes())
        else:
            self.machine = ResourceRestrainedMachine(nproc)

    def done(self):
        return self.dgraph.dep_pool_is_empty() and self.dgraph.no_dep_pool_is_empty()

    def schedule(self):
        init_set = self.machine.assign_cores(self.dgraph, self.dgraph.get_no_deps_pool())
        for x in init_set:
            node = self.dgraph.nodemap[x]
            node.starttime = 0
            node.endtime = node.time

        while not self.done():
            done_set = self.processing_until_done()
            self.machine.yield_cores(self.dgraph.nodemap, done_set)
            no_deps = self.dgraph.new_no_deps(self.current_time)
            if no_deps != set():
                self.dgraph.commit(no_deps)
                self.machine.assign_cores(self.dgraph, no_deps)

        # Exercise 1
        # 1. find the critical path (the processor that takes longer to done the job) of each graph
        # 2. Find the minimum finite number of processors needed to execute (same executing time)
        if self.nproc == 0:
            print("There are", self.tasks_on_critical_path(), "tasks on critical path")
            print("Minimum finite number of processors needed to execute: ",
                  self.machine.core_usage())

    # return the set of tasks that has been done since having the shortest executing time
    # the side effect is
    # 1) updating the executing time in nodes in nodemap
    # 2) update current_time
    def processing_until_done(self):
        min_time = self.dgraph.find_shortest_time()
        local_done_set = self.dgraph.update_no_dep_nodes_time(min_time)
        self.current_time += min_time

        return local_done_set

    # return the amount of tasks on the critical path
    def tasks_on_critical_path(self):
        def aux(index):
            if self.dgraph.nodemap[index].endtime == longest_executing_time:
                return index

        longest_executing_time = max(list(map(lambda x: self.dgraph.nodemap[x].endtime, self.dgraph.nodemap)))

        # could have more than one critical path
        unsolved_2d_list = [[x] for x in list(filter(aux, self.dgraph.nodemap))]
        solved = set()

        while unsolved_2d_list:
            unsolved = unsolved_2d_list.pop(0)
            local_done = []
            while unsolved:
                done = unsolved.pop(0)
                local_done.append(done)
                solved.add(done)

                dep_set = self.dgraph.nodemap[done].dep_set
                dep_endtime_list = list(map(lambda x: self.dgraph.nodemap[x].endtime, dep_set))

                if dep_endtime_list:
                    max_end_time = max(dep_endtime_list)
                    for x in dep_set:
                        # only append deps with maximum end time
                        if self.dgraph.nodemap[x].endtime == max_end_time:
                            unsolved.append(x)
            print 'len: ', len(local_done), local_done

        return len(solved)

    def write(self, filename):
        with open(filename, "w") as f:
            for k, v in self.dgraph.nodemap.items():
                if isinstance(v.processor, int):
                    line = str(k) + " " + str(v.processor) + " " + str(v.starttime) + "\n"
                else:
                    line = str(k) + " " + str(v.processor.id) + " " + str(v.starttime) + "\n"
                f.writelines(line)


if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Offline scheduling")
    p.add_argument("taskgraph")
    p.add_argument("output")
    p.add_argument("nproc", help="Processors, 0 for infinite", default=0, type=int)

    args = p.parse_args()

    depGraph = DepGraph(args.taskgraph)
    schd = Scheduler(depGraph, args.nproc)
    schd.schedule()
    schd.write(args.output)
