#!/usr/bin/env python

import argparse
from random import shuffle

class DepNode:
    def __init__(self):
        self.time = 0
        self.ndeps = 0
        self.dep_set = set()  # set that this node depends on
        self.starttime = -1
        self.endtime = -1
        self.processor = -1

    def set(self, time, ndeps):
        self.time = time
        self.ndeps = ndeps


class DepGraph:
    def __init__(self, filename):
        f = open(filename, 'r')
        firstline = f.readline().split()

        assert len(firstline) == 2
        nodes, edges = [(int(x)) for x in firstline]

        # init all instance variables
        self.nodemap = {}
        for x in range(0, nodes):
            self.nodemap[x] = DepNode()
        self.nodes = nodes
        self.edges = edges
        self.deps_pool = set()
        self.no_deps_pool = set()
        self.done_set = set()

        # read line from file
        for line in f:
            node_arr = line.split()
            taskid = int(node_arr[0])
            time = int(node_arr[1])
            ndeps = int(node_arr[2])
            for x in node_arr[3:]:
                self.nodemap[int(x)].dep_set.add(taskid)
                self.deps_pool.add(int(x))
            self.nodemap[taskid].set(time, ndeps)

        # assume node number is sequential and always start from zero
        for x in range(0, self.nodes):
            self.no_deps_pool.add(x)

        self.no_deps_pool -= self.deps_pool

        assert len(self.nodemap) == nodes

    # check all nodes with dependences with done_set
    # return nodes with no deps
    def new_no_deps(self, starttime):
        ret = set()
        for x in self.deps_pool:
            temp = self.nodemap[x]
            if self.done_set.issuperset(temp.dep_set):
                ret.add(x)
        for x in ret:
            node = self.nodemap[x]
            node.starttime = starttime
            node.endtime = starttime + node.time
        return ret

    def commit(self, no_deps):
        self.no_deps_pool |= no_deps
        self.deps_pool -= no_deps

    def rollback(self, unavail_set):
        self.no_deps_pool -= unavail_set
        self.deps_pool |= unavail_set


class Machine:
    def __init__(self, n):
        self.cores = list(range(0, n))  # use cores as LIFO queue
        self.max = 0
        self.strategy = self.shortest_time_first_scheduling

    # assign one task to first core in queue
    def pop(self):
        assert len(self.cores) > 0

        assigned_core = self.cores.pop(0)
        if self.max < assigned_core:
            self.max = assigned_core
        return assigned_core

    # assign tasks from assign_set
    # return list of task index that has been assigned the task
    def assign_cores(self, dgraph, assign_set):
        nodemap = dgraph.nodemap

        if len(assign_set) <= len(self.cores):
            for x in assign_set:
                p = self.pop()
                nodemap[x].processor = p
            return assign_set
        else:
            avail_set = self.strategy(nodemap, assign_set)
            unavail_set = assign_set - avail_set
            dgraph.rollback(unavail_set)

            return avail_set

    # yielding the core by inserting it back to core list
    def push(self, n):
        self.cores.insert(0, n)
        assert self.cores[0] >= 0

    # yield all the cores that has done their task
    def yield_cores(self, nodemap, done_set):
        for x in done_set:
            node = nodemap[x]
            self.push(node.processor)
            assert node.processor >= 0

    def core_usage(self):
        # plus one for core-0
        return self.max + 1

    # below is strategies for assign cores to tasks

    def base_scheduling(self, nodemap, task_queue):
        avail_set = set()
        for x in task_queue:
            if len(self.cores) > 0:
                p = self.pop()
                nodemap[x].processor = p
                avail_set.add(x)
            else:
                break
        return avail_set

    # assign cores desc using task_id
    # return avail_set
    def navie_assign(self, nodemap, assign_set):
        assign_iter = reversed(sorted(list(assign_set)))
        return self.base_scheduling(nodemap, assign_iter)

    # assign cores to tasks with longest executing time
    # return avail_set
    def longest_time_first_scheduling(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: nodemap[x].time)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks with shortest executing time
    # return avail_set
    def shortest_time_first_scheduling(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: -nodemap[x].time)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks using random task_id
    # return avail_set
    def random_schduling(self, nodemap, assign_set):
        assign_list = list(assign_set)
        shuffle(assign_list)
        return self.base_scheduling(nodemap, assign_list)

    # assign cores to tasks with fewest deps
    def dep_based_scheduling(self, nodemap, assign_set):
        assign_list = sorted(list(assign_set), key=lambda x: len(nodemap[x].dep_set))
        return self.base_scheduling(nodemap, assign_list)

class Scheduler:
    def __init__(self, dgraph, nproc):
        self.dgraph = dgraph
        self.nproc = nproc
        self.current_time = 0

        # assume that use nodes as core num is the same as use infinite nodes as core num
        if nproc == 0:
            self.machine = Machine(self.dgraph.nodes)
        else:
            self.machine = Machine(nproc)

    def done(self):
        return self.dgraph.no_deps_pool == set() and self.dgraph.deps_pool == set()

    def schedule(self):
        init_set = self.machine.assign_cores(self.dgraph, self.dgraph.no_deps_pool)
        for x in init_set:
            node = self.dgraph.nodemap[x]
            node.starttime = 0
            node.endtime = node.time

        while not self.done():
            done_set = self.processing_until_done()
            self.machine.yield_cores(self.dgraph.nodemap, done_set)
            no_deps = self.dgraph.new_no_deps(self.current_time)
            if no_deps != set():
                self.dgraph.commit(no_deps)
                self.machine.assign_cores(self.dgraph, no_deps)

        # Exercise 1
        # 1. find the critical path (the processor that takes longer to done the job) of each graph
        # 2. Find the minimum finite number of processors needed to execute (same executing time)
        if self.nproc == 0:
            print("There are", self.tasks_on_critical_path(), "tasks on critical path")
            print("Minimum finite number of processors needed to execute: ",
                  self.machine.core_usage())

    # return the set of tasks that has been done since having the shortest executing time
    # the side effect is
    # 1) updating the executing time in nodes in nodemap
    # 2) update current_time
    def processing_until_done(self):
        dgraph = self.dgraph
        time_iter = map(lambda x: dgraph.nodemap[x].time, dgraph.no_deps_pool)
        min_time = min(time_iter)
        if min_time < 0:
            min_time = 0
        local_done_set = set()

        # update all nodes in non_dep set with minimum executing time
        for x in dgraph.no_deps_pool:
            dgraph.nodemap[x].time -= min_time
            if dgraph.nodemap[x].time == 0:
                dgraph.done_set.add(x)
                local_done_set.add(x)

        dgraph.no_deps_pool -= dgraph.done_set

        self.current_time += min_time

        return local_done_set

    # return the amount of tasks on the critical path
    def tasks_on_critical_path(self):
        def aux(index):
            if self.dgraph.nodemap[index].endtime == longest_executing_time:
                return index

        longest_executing_time = max(list(map(lambda x: self.dgraph.nodemap[x].endtime, self.dgraph.nodemap)))

        # could have more than one critical path
        unsolved = list(filter(aux, self.dgraph.nodemap))
        solved = []

        while unsolved:
            done = unsolved.pop(0)
            solved.append(done)

            dep_set = self.dgraph.nodemap[done].dep_set
            dep_endtime_list = list(map(lambda x: self.dgraph.nodemap[x].endtime, dep_set))

            if dep_endtime_list:
                max_end_time = max(dep_endtime_list)
                for x in dep_set:
                    # only append deps with maximum end time
                    if self.dgraph.nodemap[x].endtime == max_end_time:
                        unsolved.append(x)

        return len(solved)

    def write(self, filename):
        with open(filename, "w") as f:
            for k, v in self.dgraph.nodemap.items():
                line = str(k) + " " + str(v.processor) + " " + str(v.starttime) + "\n"
                f.writelines(line)


if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Offline scheduling")
    p.add_argument("taskgraph")
    p.add_argument("output")
    p.add_argument("nproc", help="Processors, 0 for infinite", default=0, type=int)

    args = p.parse_args()

    depGraph = DepGraph(args.taskgraph)
    schd = Scheduler(depGraph, args.nproc)
    schd.schedule()
    schd.write(args.output)
